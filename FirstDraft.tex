\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[style=alphabetic, backend=biber]{biblatex}

\graphicspath{ {./images/} }

\addbibresource{references.bib}

% Preamble
\setlength{\parindent}{0pt}

% Document Information
\title{Optimal Method of Ray Tracing Voxels with Hardware Acceleration in Real-Time}
\author{Srijan Dhungana}
\date{2024}

% Begin Document
\begin{document}

% Title Page
\maketitle
\clearpage

% Table of Contents
\tableofcontents
\clearpage


\clearpage

\section{Motivation}

My first introduction to ray tracing was when NVIDIA announced their RTX series GPUs in 2018.
The idea of simulating light rays in real-time was unreal to me and the results were stunning.
I wanted to make my own hardware ray tracer and so I was introduced to computer graphics.
I have always been fascinated by the aesthetics of voxels based games and I wanted to combine the two.
However, when researching, I found that there was not much information on the fastest method to ray trace a voxel scene with hardware acceleration.
This essay is an attempt to answer that question.

\section{Introduction}

\subsection{Ray Tracing}

Ray tracing is a method to calculate where a given ray (line), intersects
with the environment. This is useful to simulate light rays because of how light behaves. It is used in film
production, video games, optics, medical imaging, architectural visualization,
and many other fields. Physically light rays bounce in the environment, thus
ray tracing is used to calculate the position of intersection where a light ray
hits the environment. Then the programmer is free to repeat the process
and simulate the ray bouncing off into other directions. This process is a
highly parallelizable process, thus it is perfectly suited for GPUs which have
thousands of cores. However, due to how computationally expensive the
process is, it is not feasible to do in real-time without dedicated hardware
acceleration. Due to those limitations, it was only used in non-real-time
applications such as film production and pre-rendered graphics. Hardware
acceleration ray tracing was introduced in 2018 by Nvidia. Since then, ray
tracing has been possible in real-time and further supported by AMD and
Intel GPUs. This has allowed real-time applications to leverage ray-tracing.

\subsection{Voxels}

Voxels are a way to represent 3D data. It is similar to pixels in 2D images,
but in 3D space. Essentially, voxels are cubes that are located in 3D space.
They are used in medical imaging, simulations, and video games; they have
similar applications to ray tracing. Voxels are convenient, because they rep-
resent volumetric data, which is difficult to represent with polygons. For
example, a physics simulation of a fluid would be difficult to represent with
polygons, but easy with voxels. Each voxel can store data such as color,
density, temperature, etc. at each point in 3D space. The resolution of the
simulation can also be adjusted easily just by changing the size of the voxels.

\subsection{Research Question}

The research question is: What is the difference in performance between
purely hardware based voxel ray tracing and a mixed software-hardware approach in rendering?

\section{Background Information}

\subsection{Mathematics of Ray Tracing}
\subsubsection{The Ray Equation}

A ray is defined by a point and a direction. The equation of a ray is given by:

\begin{equation}
    R(t) = \vec{o} + t\vec{d}
\end{equation}

where $\vec{o}$ is the origin of the ray, $\vec{d}$ is the direction of the ray, and $t$ is the distance from the origin.
The function $R(t)$ gives the position of the ray at distance $t$ from the origin.
With this equation, any point along the ray can calculated.

\subsubsection{The Intersection Function}

Whenever a ray intersects with a surface, the point of intersection is given by the equation:

\begin{equation}
    R(t) = I(t)
\end{equation}

where $I(t)$ is the intersection function of the surface. To locate the point of intersection,
solving for $t$ is necessary by equating the ray equation with the intersection function. 
If there are solutions for $t$, then the ray intersects with the surface. However, 
if $t$ is negative, then the intersection is opposite to the direction of the ray, which is not desired,
hence negative solutions are discarded. The closest hit is usually desired thus smallest non-negative $t$ is chosen.

Without any optimization algorithms, the intersection test has to happen for every primitive in the scene to determine the closest intersection. Primitives are smallest building blocks of the scene, for example triangles, spheres, and cubes.
Computing intersection tests for every primitive is computationally expensive for large scenes, so acceleration structures (AS) are used to decrease the number of intersection tests significantly.
These structures are usually bounding volume hierarchies (BVH) that subdivide the scene into smaller parts until the smallest primitives are reached \parencite{NVIDIA:Raytracing}.
This method logarithmically decreases the number of intersection tests like a binary search algorithm.
\begin{figure}[h]
    \includegraphics[scale=0.22]{BVH-Visualization}
    \caption{
        Bounding Volume Hierarchy of a bunny. 
        The bunny is divided into smaller bounding volumes until the individual triangles are reached.
        \parencite{Medium:BVH-Visualization}}
    \label{fig:BVH-Visualization}
\end{figure}

The BVH is a non-trivial tree structure that is constructed every object in the scene and requires a lot of memory, because
it stores the volumes in addition to the objects.
When a ray enters a bounding volume, it is guaranteed that only the primitives inside the bounding volume are potential hits.
If the ray does not hit any primitives inside the bounding volume, then the ray goes to $n-1$ level of the hierarchy and continues
to search for hits. This is repeated until the ray hits a primitive or the ray exits the scene.

\subsection{Ray Tracing Algorithm}

The ray tracing algorithm is a recursive algorithm that calculates the color of a pixel by tracing rays into the scene.
The scene has a virtual camera that shoots rays into the scene.
When tracing from the camera, the algorithm can simulate the exact paths but it only simulates paths that reach the camera,
which is efficient because it avoids simulating paths that do not reach the camera.
The algorithm is as follows for each pixel in the image:

\begin{algorithm}[H]
\caption{Ray Tracing Algorithm}
\label{alg:TraceRay}
\begin{algorithmic}[1]

\State $Emissive \gets 0$
\Procedure{TraceRay}{$direction, depth$}

    \If{$depth = 0$}
        \State \Return $0$
    \EndIf

    \State $closestHit \gets \infty$
    \State $hitObject \gets null$

    \For{$object$ in $scene$}
        \State $hit \gets object.Intersect(direction)$
        \If{$hit < closestHit$}
            \State $closestHit \gets hit$
            \State $hitObject \gets object$
        \EndIf
    \EndFor

    \If{$closestHit = \infty$}
        \State $Emissive \gets 1$ \Comment {Sky is a light source}
        \State \Return $background$
    \EndIf
    \State $Emissive \gets hitObject.emissive$
    \If {$hitObject.emissive$} \Comment {If the object is a light source}
        \State \Return $hitObject.Color$
    \EndIf

    \State $newDirection \gets randomDirectionFromObject(hitObject)$

    \State \Return $object.Color * TraceRay(newDirection, depth - 1)$

\EndProcedure

\For{$pixel$ in $image$}
    \State $direction \gets pixelToDirection(pixel)$
    \State $color \gets TraceRay(direction, maxDepth) * Emissive$
    \State $image[pixel] \gets color$
    \State $Emissive \gets 0$ \Comment{Reset Emissive}
\EndFor

\end{algorithmic}
\end{algorithm}

The pseudocode in Algorithm \ref{alg:TraceRay} shows a simplified recursive algorithm for ray tracing.
It finds the closest hit in the scene and if the object is emissive (light source), then the pixel is illuminated.
Note that if the path doesn't hit a light source, the path color is multiplied by 0, which results in a black pixel.

This recursive algorithm is repeated thousands of times for each pixel and the result is an image.
Even then, the image is an approximation of the real world because the algorithm only simulates a fraction of all the possible paths.

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.22]{RayTracingImage}
    \end{center}
    \caption{Simplified visualization of the Algorithm \ref{alg:TraceRay}. \parencite{NVIDIA:Raytracing}}
    \label{fig:RayTracingImage}
\end{figure}

\subsection{GPU Architecture}

% TODO: Explain how GPUs work and how they are used in ray tracing. Also some 3D vs 1D for SDFs maybe?

\section{Methods}

Below are the two methods that will be compared in this essay.

\subsection{Hardware Acceleration Structure Traversal}

To use an AS it needs to be constructed for the objects in the scene.
Unfortunately, acceleration structure generation is a black box in graphics APIs. 
The user supplies the object's primitives and the GPU driver constructs an AS.
These primitives are usually triangles or axis aligned bounding boxes (AABB).
AABBs are cuboids that are aligned in the x, y, and z axes, hence the name axis aligned.
GPUs that support hardware accelerated ray tracing are extremely optimized for triangles and AABB intersection tests.
Even triangles are encosed in AABBs in a BVH.

Cubes can be represented as AABBs, therefore voxels can be represented as AABBs.
This is useful because the GPU can construct the AS without the user having to do it.
However, the AABBs are not inteded to use as voxels, instead they are a flexible way to enclose user defined objects.
For example, a smooth sphere would not be possible to represent using triangles, but there 
is an intersection function that would render a smoother sphere than triangles ever could.
AABBs are used to enclose the sphere and the GPU will notify the user for any potential hits with the sphere.
This can be done for anything as long as it's enclosed in an AABB.
Because of this, they are not optimized for the voxel use case, where there would be millions of
densely packed AABBs in a scene.

Additionally, 

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.22]{ClosestHit-Approx}
    \end{center}
    \caption{
        Solving for closest hit candidate without an intersection test.
        Instead the distance from the ray origin to the AABB is calculated and the closest candidate is selected.}
    \label{fig:ClosestHit-Approx}
\end{figure}

There are caveats to using hardware acceleration for voxel ray tracing.
The GPU does not solve for the closest intersection for AABBs, instead it only tells the user if there is a potential hit \parencite{DirectX:Specification}.
Closest hit has to be solved by the user, since the GPU expects the enclosed object to have a different surface than the AABB.
It is obvious that the GPU is not optimized for voxel ray tracing.
However, the GPU provides a transformation matrix that stores the location of the AABB.
Since expensive intersection tests should be avoided, instead of solving for the intersection, only the distance from the ray origin to the potential AABB can be calculated.
Figure \ref{fig:ClosestHit-Approx} shows solving for the closest candiate without an intersection test.
This much more efficient than solving for the intersection and it ensures the closest candidate is chosen.
After finding the closest candidate, the user can solve for the intersection and determine where the ray hits the voxel.

\subsection{Signed Distance Fields}

Signed distance fields (SDF) used to store distance information to the nearest surface. 
SDFs work like LiDAR, where the sensor measures the distance to the nearest surface to map out the environment.
SDFs are used in ray marching which is a technique to render surfaces. 
Ray marching is a technique where the ray is stepped in small increments and the distance to the nearest surface is calculated.
If the distance is less than a threshold, then the ray has hit the surface, this avoids expensive intersection tests for every object in the scene.
However, the ray has to take steps through the scene which can add up to thousands of steps for a single ray.
This may have worse performance if the signed distance field cannot skip large empty spaces in the scene, thus
requiring more steps to reach the surface.

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.22]{Voxel-SDF}
    \end{center}
    \caption{
        2D visualization of a SDF. The black areas are empty space and store a positive distance to the nearest surface.
        The white areas are the surface thus store 0 as their distance to the nearest surface.
        \parencite{ByteWrangler:Voxel-SDF}}
    \label{fig:Voxel-SDF}
\end{figure}

The SDF approach in this essay will use a mix of software and hardware acceleration.
Storing the whole scene as a SDF would consume a lot of memory because most of space is empty.
Figure \ref{fig:Voxel-SDF} shows that storing a scene as a SDF would be wasteful because of empty space.
This essay's method will enclose the object tightly. For example, in figure \ref{fig:Voxel-SDF}, an AABB would enclose a 2x2 tightly packed SDF instead of 6x6 which has a lot of empty space.
Hardware accelerated AABBs will enclose the SDFs to skip vast empty spaces in the scene.
Only a certain amount of bits will be used to store the distance to the nearest surface in the SDF.
Additionally, ray marching is not hardware accelerated, so it will be written in software.

\section{Memory Consumption}

\subsection{Hardware Acceleration Structure Traversal}

The memory consumption is greater for AABBs than for voxels.
Unfortunately, the GPU requires AABBs to be used for hardware acceleration.
AABBs need two 3D vectors to represent the minimum and maximum points of the cuboid.
This is 6 floats or 24 bytes per AABB.
If there is a scene with 1 million voxels, then the memory consumption would be 24MB.
This may not sound like much, however the AS has extra AABBs for the hierarchy, which will add upto more memory.
To do lighting calculations and other operations, the user would need to access the data inside the voxels.
The graphics API does not allow accessing the data inside the AS, so there would be duplicated data.
One for the AS and one for the user to access.

\subsection{Signed Distance Fields}

SDF memory consumption is notably less than AABBs.
They can be represented as a 3D grid. Each cell in the grid stores the distance to the nearest surface.
Therefore, cells with a distance of 0 are on the surface. There is no need to store the positions since
every cell is a fixed distance from each other. and the user can calculate the position of the cell by using the cell's x, y, z index in the 3D grid.
This saves 24 bytes per voxel, when compared to AABBs and there is no need to maintain a hierarchy containing extra AABBs.
Additionally, there is no need to duplicate the data since the user can access the data directly.
SDFs take up as much space as the user needs to store the scene. The SDF requires the distance and user defined extra voxel information.
The extra information can be an index to another array containing the material. 
The drawback is that the empty spaces also take as much space.
However, by minimizing emptry space and tightly enclosing the object, the memory consumption can be reduced further.
Partitioning a model further into smaller SDFs can reduce memory consumption as that would reduce the empty space.

\section{Results}

\subsection{Hardware Acclerated Method}
\subsection{Signed Distance Fields Method}

\section{Conclusion}

\printbibliography

\end{document}